Human vocabulary comes in free text. 
In order to make a machine learning model understand and process the natural language, we need to transform the free-text words into numeric values.
One of the simplest transformation approaches is to do a one-hot encoding in which each distinct word stands for one dimension of the resulting vector and a binary value indicates whether the word presents (1) or not (0).
However, one-hot encoding is impractical computationally when dealing with the entire vocabulary, as the representation demands hundreds of thousands of dimensions.
 Word embedding represents words and phrases in vectors of (non-binary) numeric values with much lower and thus denser dimensions. 
An intuitive assumption for good word embedding is that they can approximate the similarity between words (i.e., “cat” and “kitten” are similar words, and thus they are expected to be close in the reduced vector space) or disclose hidden semantic relationships (i.e., the relationship between “cat” and “kitten” is an analogy to the one between “dog” and “puppy”).
 Contextual information is super useful for learning word meaning and relationship, as similar words may appear in the similar context often.
